{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=pd.read_csv('cc.csv',names=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1         2         3         4         5         6         7   \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         8         9         10  ...        21        22        23        24  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "0  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "1  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "1  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "2 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "         25        26        27        28      29  30  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62   0  \n",
       "0  0.167170  0.125895 -0.008983  0.014724    2.69   0  \n",
       "1 -0.327642 -0.139097 -0.055353 -0.059752  378.66   0  \n",
       "1  0.647376 -0.221929  0.062723  0.061458  123.50   0  \n",
       "2 -0.206010  0.502292  0.219422  0.215153   69.99   0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>663.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.173477</td>\n",
       "      <td>0.202923</td>\n",
       "      <td>0.868027</td>\n",
       "      <td>0.313785</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.191067</td>\n",
       "      <td>0.120179</td>\n",
       "      <td>-0.057877</td>\n",
       "      <td>-0.021444</td>\n",
       "      <td>0.050286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011559</td>\n",
       "      <td>-0.101324</td>\n",
       "      <td>-0.039138</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.118359</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.026277</td>\n",
       "      <td>-0.015521</td>\n",
       "      <td>70.339698</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.318746</td>\n",
       "      <td>1.195083</td>\n",
       "      <td>1.017452</td>\n",
       "      <td>1.268028</td>\n",
       "      <td>1.131101</td>\n",
       "      <td>1.248768</td>\n",
       "      <td>0.860575</td>\n",
       "      <td>0.828433</td>\n",
       "      <td>0.897497</td>\n",
       "      <td>0.989299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611034</td>\n",
       "      <td>0.609244</td>\n",
       "      <td>0.369201</td>\n",
       "      <td>0.628296</td>\n",
       "      <td>0.426569</td>\n",
       "      <td>0.468524</td>\n",
       "      <td>0.308202</td>\n",
       "      <td>0.291046</td>\n",
       "      <td>206.447777</td>\n",
       "      <td>0.194254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6.093248</td>\n",
       "      <td>-12.114213</td>\n",
       "      <td>-5.694973</td>\n",
       "      <td>-4.657545</td>\n",
       "      <td>-6.631951</td>\n",
       "      <td>-3.498447</td>\n",
       "      <td>-4.925568</td>\n",
       "      <td>-7.494658</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>-2.772272</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.134608</td>\n",
       "      <td>-2.776923</td>\n",
       "      <td>-3.553381</td>\n",
       "      <td>-1.867208</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>-1.243924</td>\n",
       "      <td>-2.377933</td>\n",
       "      <td>-2.735623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.901810</td>\n",
       "      <td>-0.174201</td>\n",
       "      <td>0.312139</td>\n",
       "      <td>-0.436332</td>\n",
       "      <td>-0.552683</td>\n",
       "      <td>-0.613937</td>\n",
       "      <td>-0.324460</td>\n",
       "      <td>-0.157950</td>\n",
       "      <td>-0.517152</td>\n",
       "      <td>-0.371514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219877</td>\n",
       "      <td>-0.528465</td>\n",
       "      <td>-0.172574</td>\n",
       "      <td>-0.384946</td>\n",
       "      <td>-0.160158</td>\n",
       "      <td>-0.313228</td>\n",
       "      <td>-0.045075</td>\n",
       "      <td>-0.015932</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.376162</td>\n",
       "      <td>0.277666</td>\n",
       "      <td>0.897601</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>-0.107492</td>\n",
       "      <td>-0.068701</td>\n",
       "      <td>0.117003</td>\n",
       "      <td>0.044961</td>\n",
       "      <td>-0.074016</td>\n",
       "      <td>-0.081320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070069</td>\n",
       "      <td>-0.069830</td>\n",
       "      <td>-0.045366</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.145526</td>\n",
       "      <td>-0.015710</td>\n",
       "      <td>0.023011</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.108861</td>\n",
       "      <td>0.877669</td>\n",
       "      <td>1.509547</td>\n",
       "      <td>1.140908</td>\n",
       "      <td>0.466815</td>\n",
       "      <td>0.490715</td>\n",
       "      <td>0.567376</td>\n",
       "      <td>0.262158</td>\n",
       "      <td>0.415102</td>\n",
       "      <td>0.273067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102520</td>\n",
       "      <td>0.311334</td>\n",
       "      <td>0.073547</td>\n",
       "      <td>0.429625</td>\n",
       "      <td>0.439774</td>\n",
       "      <td>0.263885</td>\n",
       "      <td>0.115571</td>\n",
       "      <td>0.087914</td>\n",
       "      <td>57.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.586093</td>\n",
       "      <td>5.267376</td>\n",
       "      <td>3.772857</td>\n",
       "      <td>4.075817</td>\n",
       "      <td>7.672544</td>\n",
       "      <td>5.122103</td>\n",
       "      <td>4.808426</td>\n",
       "      <td>1.726413</td>\n",
       "      <td>5.459274</td>\n",
       "      <td>8.821756</td>\n",
       "      <td>...</td>\n",
       "      <td>5.273420</td>\n",
       "      <td>1.461535</td>\n",
       "      <td>3.150413</td>\n",
       "      <td>1.215279</td>\n",
       "      <td>1.136720</td>\n",
       "      <td>3.065576</td>\n",
       "      <td>2.490503</td>\n",
       "      <td>1.575380</td>\n",
       "      <td>3828.040000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1           2           3           4           5           6   \\\n",
       "count  663.000000  663.000000  663.000000  663.000000  663.000000  663.000000   \n",
       "mean    -0.173477    0.202923    0.868027    0.313785    0.004962    0.191067   \n",
       "std      1.318746    1.195083    1.017452    1.268028    1.131101    1.248768   \n",
       "min     -6.093248  -12.114213   -5.694973   -4.657545   -6.631951   -3.498447   \n",
       "25%     -0.901810   -0.174201    0.312139   -0.436332   -0.552683   -0.613937   \n",
       "50%     -0.376162    0.277666    0.897601    0.448154   -0.107492   -0.068701   \n",
       "75%      1.108861    0.877669    1.509547    1.140908    0.466815    0.490715   \n",
       "max      1.586093    5.267376    3.772857    4.075817    7.672544    5.122103   \n",
       "\n",
       "               7           8           9           10  ...          21  \\\n",
       "count  663.000000  663.000000  663.000000  663.000000  ...  663.000000   \n",
       "mean     0.120179   -0.057877   -0.021444    0.050286  ...    0.011559   \n",
       "std      0.860575    0.828433    0.897497    0.989299  ...    0.611034   \n",
       "min     -4.925568   -7.494658   -2.770089   -2.772272  ...   -4.134608   \n",
       "25%     -0.324460   -0.157950   -0.517152   -0.371514  ...   -0.219877   \n",
       "50%      0.117003    0.044961   -0.074016   -0.081320  ...   -0.070069   \n",
       "75%      0.567376    0.262158    0.415102    0.273067  ...    0.102520   \n",
       "max      4.808426    1.726413    5.459274    8.821756  ...    5.273420   \n",
       "\n",
       "               22          23          24          25          26          27  \\\n",
       "count  663.000000  663.000000  663.000000  663.000000  663.000000  663.000000   \n",
       "mean    -0.101324   -0.039138   -0.001128    0.118359    0.023800    0.026277   \n",
       "std      0.609244    0.369201    0.628296    0.426569    0.468524    0.308202   \n",
       "min     -2.776923   -3.553381   -1.867208   -1.389079   -1.243924   -2.377933   \n",
       "25%     -0.528465   -0.172574   -0.384946   -0.160158   -0.313228   -0.045075   \n",
       "50%     -0.069830   -0.045366    0.091900    0.145526   -0.015710    0.023011   \n",
       "75%      0.311334    0.073547    0.429625    0.439774    0.263885    0.115571   \n",
       "max      1.461535    3.150413    1.215279    1.136720    3.065576    2.490503   \n",
       "\n",
       "               28           29          30  \n",
       "count  663.000000   663.000000  663.000000  \n",
       "mean    -0.015521    70.339698    0.039216  \n",
       "std      0.291046   206.447777    0.194254  \n",
       "min     -2.735623     0.000000    0.000000  \n",
       "25%     -0.015932     5.980000    0.000000  \n",
       "50%      0.022966    17.240000    0.000000  \n",
       "75%      0.087914    57.750000    0.000000  \n",
       "max      1.575380  3828.040000    1.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams ['figure.figsize']=(10,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f16ec41cc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     -1.023190\n",
       "2     -2.598730\n",
       "3     -0.895776\n",
       "4     -0.277100\n",
       "5      0.612589\n",
       "6      1.577578\n",
       "7      0.349123\n",
       "8     -3.779956\n",
       "9      1.118661\n",
       "10     3.811942\n",
       "11     0.151029\n",
       "12    -0.484747\n",
       "13    -0.165198\n",
       "14    -2.643770\n",
       "15    -0.666142\n",
       "16    -0.676734\n",
       "17    -0.269912\n",
       "18     0.007315\n",
       "19    -0.005649\n",
       "20     4.524971\n",
       "21     3.162454\n",
       "22    -0.243161\n",
       "23    -0.178004\n",
       "24    -0.619881\n",
       "25    -0.403746\n",
       "26     0.903001\n",
       "27     0.483061\n",
       "28    -3.248704\n",
       "29    10.771276\n",
       "30     4.758490\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc[30].replace('yes',0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc[30].replace('no',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=np.abs(cc.iloc[:,0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_data=cc[(z<3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f16f18c710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_data.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(663, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.131544</td>\n",
       "      <td>0.611829</td>\n",
       "      <td>0.792038</td>\n",
       "      <td>0.387829</td>\n",
       "      <td>0.117791</td>\n",
       "      <td>-0.399963</td>\n",
       "      <td>0.222929</td>\n",
       "      <td>-0.039608</td>\n",
       "      <td>-0.176672</td>\n",
       "      <td>-0.145581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076397</td>\n",
       "      <td>-0.191475</td>\n",
       "      <td>-0.004662</td>\n",
       "      <td>0.030905</td>\n",
       "      <td>0.027682</td>\n",
       "      <td>0.080247</td>\n",
       "      <td>0.031574</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>1.507075</td>\n",
       "      <td>0.037736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.038856</td>\n",
       "      <td>0.576061</td>\n",
       "      <td>0.697366</td>\n",
       "      <td>0.940682</td>\n",
       "      <td>0.654729</td>\n",
       "      <td>0.751551</td>\n",
       "      <td>0.499200</td>\n",
       "      <td>0.507201</td>\n",
       "      <td>0.502867</td>\n",
       "      <td>0.604173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341602</td>\n",
       "      <td>0.524644</td>\n",
       "      <td>0.174069</td>\n",
       "      <td>0.547821</td>\n",
       "      <td>0.434990</td>\n",
       "      <td>0.366926</td>\n",
       "      <td>0.220756</td>\n",
       "      <td>0.154314</td>\n",
       "      <td>0.724084</td>\n",
       "      <td>0.191462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.008872</td>\n",
       "      <td>-1.174338</td>\n",
       "      <td>-0.793312</td>\n",
       "      <td>-2.378536</td>\n",
       "      <td>-1.511234</td>\n",
       "      <td>-1.547978</td>\n",
       "      <td>-1.308064</td>\n",
       "      <td>-2.764262</td>\n",
       "      <td>-1.706012</td>\n",
       "      <td>-1.418769</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150128</td>\n",
       "      <td>-0.980511</td>\n",
       "      <td>-0.547134</td>\n",
       "      <td>-1.397547</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>-0.553471</td>\n",
       "      <td>-0.904113</td>\n",
       "      <td>-0.668827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.735140</td>\n",
       "      <td>0.254437</td>\n",
       "      <td>0.304157</td>\n",
       "      <td>0.056485</td>\n",
       "      <td>-0.377073</td>\n",
       "      <td>-0.925891</td>\n",
       "      <td>-0.036715</td>\n",
       "      <td>-0.200610</td>\n",
       "      <td>-0.483655</td>\n",
       "      <td>-0.482474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240718</td>\n",
       "      <td>-0.639382</td>\n",
       "      <td>-0.105684</td>\n",
       "      <td>-0.362773</td>\n",
       "      <td>-0.221439</td>\n",
       "      <td>-0.190312</td>\n",
       "      <td>-0.026591</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.151941</td>\n",
       "      <td>0.510814</td>\n",
       "      <td>0.667144</td>\n",
       "      <td>0.524755</td>\n",
       "      <td>0.084975</td>\n",
       "      <td>-0.587852</td>\n",
       "      <td>0.140268</td>\n",
       "      <td>-0.032182</td>\n",
       "      <td>-0.133681</td>\n",
       "      <td>-0.200723</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139384</td>\n",
       "      <td>-0.247018</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.158054</td>\n",
       "      <td>0.098117</td>\n",
       "      <td>0.091969</td>\n",
       "      <td>0.035758</td>\n",
       "      <td>0.020633</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.199336</td>\n",
       "      <td>1.014032</td>\n",
       "      <td>1.293636</td>\n",
       "      <td>0.897728</td>\n",
       "      <td>0.542384</td>\n",
       "      <td>0.069457</td>\n",
       "      <td>0.588140</td>\n",
       "      <td>0.178696</td>\n",
       "      <td>0.109736</td>\n",
       "      <td>0.010485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020278</td>\n",
       "      <td>0.206537</td>\n",
       "      <td>0.119351</td>\n",
       "      <td>0.387357</td>\n",
       "      <td>0.266775</td>\n",
       "      <td>0.178084</td>\n",
       "      <td>0.103144</td>\n",
       "      <td>0.094557</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.491574</td>\n",
       "      <td>2.198527</td>\n",
       "      <td>2.956914</td>\n",
       "      <td>2.813750</td>\n",
       "      <td>2.941968</td>\n",
       "      <td>2.955053</td>\n",
       "      <td>1.563170</td>\n",
       "      <td>1.530817</td>\n",
       "      <td>1.775956</td>\n",
       "      <td>2.303627</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429374</td>\n",
       "      <td>1.340696</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>1.013290</td>\n",
       "      <td>0.986072</td>\n",
       "      <td>1.146692</td>\n",
       "      <td>0.707519</td>\n",
       "      <td>0.250372</td>\n",
       "      <td>2.990000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1           2           3           4           5           6   \\\n",
       "count  106.000000  106.000000  106.000000  106.000000  106.000000  106.000000   \n",
       "mean     0.131544    0.611829    0.792038    0.387829    0.117791   -0.399963   \n",
       "std      1.038856    0.576061    0.697366    0.940682    0.654729    0.751551   \n",
       "min     -2.008872   -1.174338   -0.793312   -2.378536   -1.511234   -1.547978   \n",
       "25%     -0.735140    0.254437    0.304157    0.056485   -0.377073   -0.925891   \n",
       "50%     -0.151941    0.510814    0.667144    0.524755    0.084975   -0.587852   \n",
       "75%      1.199336    1.014032    1.293636    0.897728    0.542384    0.069457   \n",
       "max      1.491574    2.198527    2.956914    2.813750    2.941968    2.955053   \n",
       "\n",
       "               7           8           9           10  ...          21  \\\n",
       "count  106.000000  106.000000  106.000000  106.000000  ...  106.000000   \n",
       "mean     0.222929   -0.039608   -0.176672   -0.145581  ...   -0.076397   \n",
       "std      0.499200    0.507201    0.502867    0.604173  ...    0.341602   \n",
       "min     -1.308064   -2.764262   -1.706012   -1.418769  ...   -1.150128   \n",
       "25%     -0.036715   -0.200610   -0.483655   -0.482474  ...   -0.240718   \n",
       "50%      0.140268   -0.032182   -0.133681   -0.200723  ...   -0.139384   \n",
       "75%      0.588140    0.178696    0.109736    0.010485  ...    0.020278   \n",
       "max      1.563170    1.530817    1.775956    2.303627  ...    2.429374   \n",
       "\n",
       "               22          23          24          25          26          27  \\\n",
       "count  106.000000  106.000000  106.000000  106.000000  106.000000  106.000000   \n",
       "mean    -0.191475   -0.004662    0.030905    0.027682    0.080247    0.031574   \n",
       "std      0.524644    0.174069    0.547821    0.434990    0.366926    0.220756   \n",
       "min     -0.980511   -0.547134   -1.397547   -1.389079   -0.553471   -0.904113   \n",
       "25%     -0.639382   -0.105684   -0.362773   -0.221439   -0.190312   -0.026591   \n",
       "50%     -0.247018    0.002921    0.158054    0.098117    0.091969    0.035758   \n",
       "75%      0.206537    0.119351    0.387357    0.266775    0.178084    0.103144   \n",
       "max      1.340696    0.870300    1.013290    0.986072    1.146692    0.707519   \n",
       "\n",
       "               28          29          30  \n",
       "count  106.000000  106.000000  106.000000  \n",
       "mean     0.005107    1.507075    0.037736  \n",
       "std      0.154314    0.724084    0.191462  \n",
       "min     -0.668827    0.000000    0.000000  \n",
       "25%      0.001192    0.992500    0.000000  \n",
       "50%      0.020633    1.290000    0.000000  \n",
       "75%      0.094557    1.980000    0.000000  \n",
       "max      0.250372    2.990000    1.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    -0.157664\n",
       "2     0.062098\n",
       "3     0.400691\n",
       "4    -0.475393\n",
       "5     0.509556\n",
       "6     1.255995\n",
       "7    -0.163445\n",
       "8    -1.997146\n",
       "9     0.148759\n",
       "10    1.384960\n",
       "11    0.408607\n",
       "12   -0.187639\n",
       "13   -0.118141\n",
       "14   -1.496069\n",
       "15   -0.981720\n",
       "16   -1.041547\n",
       "17    0.575972\n",
       "18   -0.875555\n",
       "19   -0.190464\n",
       "20    1.059746\n",
       "21    3.931627\n",
       "22    0.467357\n",
       "23    0.644462\n",
       "24   -0.582536\n",
       "25   -0.636010\n",
       "26    0.665369\n",
       "27   -1.496496\n",
       "28   -2.092923\n",
       "29    0.144474\n",
       "30    4.921644\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    0\n",
       "19    0\n",
       "20    0\n",
       "21    0\n",
       "22    0\n",
       "23    0\n",
       "24    0\n",
       "25    0\n",
       "26    0\n",
       "27    0\n",
       "28    0\n",
       "29    0\n",
       "30    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=z_data[30]==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=cc[30]==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    102\n",
       "True       4\n",
       "Name: 30, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    637\n",
       "True      26\n",
       "Name: 30, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=z_data.iloc[:,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=z_data.iloc[:,-1].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=np.arange(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud=z_data[30]==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal=z_data[30]==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading https://files.pythonhosted.org/packages/81/a7/4179e6ebfd654bd0eac0b9c06125b8b4c96a9d0a8ff9e9507eb2a26d2d7e/imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/62/08c14224a7e242df2cef7b312d2ef821c3931ec9b015ff93bb52ec8a10a3/imbalanced_learn-0.5.0-py3-none-any.whl (173kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in c:\\users\\utkarsh\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\utkarsh\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17 in c:\\users\\utkarsh\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\utkarsh\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.16.4)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.5.0 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 29)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is : 1 0.92\n",
      "accuracy score is : 2 0.92\n",
      "accuracy score is : 3 0.92\n",
      "accuracy score is : 4 1.0\n",
      "accuracy score is : 5 0.96\n",
      "accuracy score is : 6 0.92\n",
      "accuracy score is : 7 0.96\n",
      "accuracy score is : 8 0.96\n",
      "accuracy score is : 9 0.92\n",
      "accuracy score is : 10 0.88\n",
      "accuracy score is : 11 0.96\n",
      "accuracy score is : 12 0.96\n",
      "accuracy score is : 13 1.0\n",
      "accuracy score is : 14 0.96\n",
      "accuracy score is : 15 1.0\n",
      "accuracy score is : 16 0.92\n",
      "accuracy score is : 17 0.92\n",
      "accuracy score is : 18 0.92\n",
      "accuracy score is : 19 0.92\n",
      "accuracy score is : 20 0.96\n",
      "accuracy score is : 21 1.0\n",
      "accuracy score is : 22 0.88\n",
      "accuracy score is : 23 0.96\n",
      "accuracy score is : 24 0.92\n",
      "accuracy score is : 25 0.96\n",
      "accuracy score is : 26 0.96\n",
      "accuracy score is : 27 1.0\n",
      "accuracy score is : 28 0.92\n",
      "accuracy score is : 29 0.96\n",
      "accuracy score is : 30 0.92\n",
      "accuracy score is : 31 1.0\n",
      "accuracy score is : 32 0.92\n",
      "accuracy score is : 33 0.96\n",
      "accuracy score is : 34 0.96\n",
      "accuracy score is : 35 0.96\n",
      "accuracy score is : 36 0.96\n",
      "accuracy score is : 37 0.96\n",
      "accuracy score is : 38 1.0\n",
      "accuracy score is : 39 1.0\n",
      "accuracy score is : 40 0.92\n",
      "accuracy score is : 41 0.96\n",
      "accuracy score is : 42 0.96\n",
      "accuracy score is : 43 0.96\n",
      "accuracy score is : 44 0.96\n",
      "accuracy score is : 45 0.96\n",
      "accuracy score is : 46 0.96\n",
      "accuracy score is : 47 0.96\n",
      "accuracy score is : 48 0.96\n",
      "accuracy score is : 49 0.92\n",
      "accuracy score is : 50 0.88\n",
      "accuracy score is : 51 0.92\n",
      "accuracy score is : 52 1.0\n",
      "accuracy score is : 53 1.0\n",
      "accuracy score is : 54 0.92\n",
      "accuracy score is : 55 1.0\n",
      "accuracy score is : 56 0.96\n",
      "accuracy score is : 57 0.96\n",
      "accuracy score is : 58 0.92\n",
      "accuracy score is : 59 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is : 60 1.0\n",
      "accuracy score is : 61 0.96\n",
      "accuracy score is : 62 0.96\n",
      "accuracy score is : 63 0.92\n",
      "accuracy score is : 64 0.96\n",
      "accuracy score is : 65 0.88\n",
      "accuracy score is : 66 0.88\n",
      "accuracy score is : 67 0.96\n",
      "accuracy score is : 68 1.0\n",
      "accuracy score is : 69 0.92\n",
      "accuracy score is : 70 0.88\n",
      "accuracy score is : 71 0.96\n",
      "accuracy score is : 72 0.96\n",
      "accuracy score is : 73 0.96\n",
      "accuracy score is : 74 1.0\n",
      "accuracy score is : 75 1.0\n",
      "accuracy score is : 76 0.92\n",
      "accuracy score is : 77 0.88\n",
      "accuracy score is : 78 0.96\n",
      "accuracy score is : 79 1.0\n",
      "accuracy score is : 80 0.92\n",
      "accuracy score is : 81 1.0\n",
      "accuracy score is : 82 0.92\n",
      "accuracy score is : 83 0.96\n",
      "accuracy score is : 84 0.92\n",
      "accuracy score is : 85 0.92\n",
      "accuracy score is : 86 0.96\n",
      "accuracy score is : 87 0.96\n",
      "accuracy score is : 88 0.96\n",
      "accuracy score is : 89 0.96\n",
      "accuracy score is : 90 0.92\n",
      "accuracy score is : 91 1.0\n",
      "accuracy score is : 92 0.92\n",
      "accuracy score is : 93 0.92\n",
      "accuracy score is : 94 0.92\n",
      "accuracy score is : 95 0.96\n",
      "accuracy score is : 96 0.92\n",
      "accuracy score is : 97 0.92\n",
      "accuracy score is : 98 0.96\n",
      "accuracy score is : 99 1.0\n",
      "accuracy score is : 100 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "for m in list:\n",
    "    xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=25,random_state=m)\n",
    "    lr.fit(xtrain,ytrain)\n",
    "    pred=lr.predict(xtest)\n",
    "    print('accuracy score is :', m+1, accuracy_score(pred,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.436905</td>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.924591</td>\n",
       "      <td>-0.727219</td>\n",
       "      <td>0.915679</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>0.707642</td>\n",
       "      <td>0.087962</td>\n",
       "      <td>-0.665271</td>\n",
       "      <td>-0.737980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194796</td>\n",
       "      <td>-0.672638</td>\n",
       "      <td>-0.156858</td>\n",
       "      <td>-0.888386</td>\n",
       "      <td>-0.342413</td>\n",
       "      <td>-0.049027</td>\n",
       "      <td>0.079692</td>\n",
       "      <td>0.131024</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.166616</td>\n",
       "      <td>0.502120</td>\n",
       "      <td>-0.067300</td>\n",
       "      <td>2.261569</td>\n",
       "      <td>0.428804</td>\n",
       "      <td>0.089474</td>\n",
       "      <td>0.241147</td>\n",
       "      <td>0.138082</td>\n",
       "      <td>-0.989162</td>\n",
       "      <td>0.922175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>-0.061972</td>\n",
       "      <td>-0.103855</td>\n",
       "      <td>-0.370415</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.108556</td>\n",
       "      <td>-0.040521</td>\n",
       "      <td>-0.011418</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.946525</td>\n",
       "      <td>-0.044901</td>\n",
       "      <td>-0.405570</td>\n",
       "      <td>-1.013057</td>\n",
       "      <td>2.941968</td>\n",
       "      <td>2.955053</td>\n",
       "      <td>-0.063063</td>\n",
       "      <td>0.855546</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0.573743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579526</td>\n",
       "      <td>-0.799229</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>0.983421</td>\n",
       "      <td>0.321201</td>\n",
       "      <td>0.149650</td>\n",
       "      <td>0.707519</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.535388</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>1.351076</td>\n",
       "      <td>0.147575</td>\n",
       "      <td>0.433680</td>\n",
       "      <td>0.086983</td>\n",
       "      <td>0.693039</td>\n",
       "      <td>0.179742</td>\n",
       "      <td>-0.285642</td>\n",
       "      <td>-0.482474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.206537</td>\n",
       "      <td>-0.187108</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.098117</td>\n",
       "      <td>-0.553471</td>\n",
       "      <td>-0.078306</td>\n",
       "      <td>0.025427</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.535388</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>1.351076</td>\n",
       "      <td>0.147575</td>\n",
       "      <td>0.433680</td>\n",
       "      <td>0.086983</td>\n",
       "      <td>0.693039</td>\n",
       "      <td>0.179742</td>\n",
       "      <td>-0.285642</td>\n",
       "      <td>-0.482474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.206537</td>\n",
       "      <td>-0.187108</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.098117</td>\n",
       "      <td>-0.553471</td>\n",
       "      <td>-0.078306</td>\n",
       "      <td>0.025427</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.452187</td>\n",
       "      <td>1.765124</td>\n",
       "      <td>0.611669</td>\n",
       "      <td>1.176825</td>\n",
       "      <td>-0.445980</td>\n",
       "      <td>0.246826</td>\n",
       "      <td>-0.257566</td>\n",
       "      <td>1.092472</td>\n",
       "      <td>-0.607524</td>\n",
       "      <td>0.047156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082280</td>\n",
       "      <td>0.325782</td>\n",
       "      <td>-0.069107</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>-0.044668</td>\n",
       "      <td>-0.243441</td>\n",
       "      <td>0.149180</td>\n",
       "      <td>0.120557</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-2.008872</td>\n",
       "      <td>2.198527</td>\n",
       "      <td>0.144242</td>\n",
       "      <td>1.159432</td>\n",
       "      <td>-0.815174</td>\n",
       "      <td>0.182288</td>\n",
       "      <td>-0.617108</td>\n",
       "      <td>1.530817</td>\n",
       "      <td>-0.586832</td>\n",
       "      <td>0.129876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094917</td>\n",
       "      <td>0.294983</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.015249</td>\n",
       "      <td>0.034211</td>\n",
       "      <td>-0.236141</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.117986</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.199356</td>\n",
       "      <td>0.129953</td>\n",
       "      <td>0.863585</td>\n",
       "      <td>1.002635</td>\n",
       "      <td>-0.783761</td>\n",
       "      <td>-0.884679</td>\n",
       "      <td>-0.040743</td>\n",
       "      <td>-0.208069</td>\n",
       "      <td>0.392478</td>\n",
       "      <td>-0.248178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042468</td>\n",
       "      <td>0.198474</td>\n",
       "      <td>-0.033010</td>\n",
       "      <td>1.013290</td>\n",
       "      <td>0.559098</td>\n",
       "      <td>0.401818</td>\n",
       "      <td>-0.005865</td>\n",
       "      <td>0.017936</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.095525</td>\n",
       "      <td>-0.116085</td>\n",
       "      <td>1.397912</td>\n",
       "      <td>1.497547</td>\n",
       "      <td>-1.049124</td>\n",
       "      <td>0.072839</td>\n",
       "      <td>-0.723802</td>\n",
       "      <td>0.287532</td>\n",
       "      <td>0.996327</td>\n",
       "      <td>-0.149145</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033234</td>\n",
       "      <td>0.093262</td>\n",
       "      <td>0.089498</td>\n",
       "      <td>0.361261</td>\n",
       "      <td>0.250963</td>\n",
       "      <td>-0.378280</td>\n",
       "      <td>0.081024</td>\n",
       "      <td>0.034227</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.295668</td>\n",
       "      <td>0.341483</td>\n",
       "      <td>0.081505</td>\n",
       "      <td>0.566746</td>\n",
       "      <td>-0.110459</td>\n",
       "      <td>-0.766325</td>\n",
       "      <td>0.073155</td>\n",
       "      <td>-0.168304</td>\n",
       "      <td>0.071837</td>\n",
       "      <td>-0.281044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323607</td>\n",
       "      <td>-0.929781</td>\n",
       "      <td>0.063809</td>\n",
       "      <td>-0.193565</td>\n",
       "      <td>0.287574</td>\n",
       "      <td>0.127881</td>\n",
       "      <td>-0.023731</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.154312</td>\n",
       "      <td>0.265462</td>\n",
       "      <td>0.384871</td>\n",
       "      <td>0.575007</td>\n",
       "      <td>-0.217475</td>\n",
       "      <td>-0.391520</td>\n",
       "      <td>-0.081489</td>\n",
       "      <td>0.062789</td>\n",
       "      <td>-0.260583</td>\n",
       "      <td>-0.161677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193213</td>\n",
       "      <td>-0.557685</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.186863</td>\n",
       "      <td>0.089252</td>\n",
       "      <td>0.093626</td>\n",
       "      <td>-0.009633</td>\n",
       "      <td>0.019668</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.948896</td>\n",
       "      <td>0.248414</td>\n",
       "      <td>2.956914</td>\n",
       "      <td>2.813750</td>\n",
       "      <td>0.145539</td>\n",
       "      <td>-0.027353</td>\n",
       "      <td>0.133702</td>\n",
       "      <td>-0.307535</td>\n",
       "      <td>-0.125244</td>\n",
       "      <td>1.034940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083647</td>\n",
       "      <td>0.416090</td>\n",
       "      <td>0.207537</td>\n",
       "      <td>0.716064</td>\n",
       "      <td>-0.602311</td>\n",
       "      <td>-0.064230</td>\n",
       "      <td>-0.315058</td>\n",
       "      <td>-0.272463</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.259873</td>\n",
       "      <td>0.254240</td>\n",
       "      <td>0.514789</td>\n",
       "      <td>0.620924</td>\n",
       "      <td>-0.475930</td>\n",
       "      <td>-0.992286</td>\n",
       "      <td>0.066417</td>\n",
       "      <td>-0.209275</td>\n",
       "      <td>0.035572</td>\n",
       "      <td>-0.067616</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256531</td>\n",
       "      <td>-0.739212</td>\n",
       "      <td>0.135732</td>\n",
       "      <td>0.381990</td>\n",
       "      <td>0.219302</td>\n",
       "      <td>0.094291</td>\n",
       "      <td>-0.028687</td>\n",
       "      <td>0.019198</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.773450</td>\n",
       "      <td>0.853112</td>\n",
       "      <td>0.818254</td>\n",
       "      <td>-0.236070</td>\n",
       "      <td>0.803463</td>\n",
       "      <td>-1.438728</td>\n",
       "      <td>0.799479</td>\n",
       "      <td>-0.007989</td>\n",
       "      <td>-0.761090</td>\n",
       "      <td>-1.044464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035362</td>\n",
       "      <td>-0.116890</td>\n",
       "      <td>-0.178926</td>\n",
       "      <td>0.400155</td>\n",
       "      <td>-0.026231</td>\n",
       "      <td>0.165156</td>\n",
       "      <td>0.027762</td>\n",
       "      <td>0.132980</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.156939</td>\n",
       "      <td>0.037215</td>\n",
       "      <td>0.556799</td>\n",
       "      <td>0.519507</td>\n",
       "      <td>-0.479754</td>\n",
       "      <td>-0.352714</td>\n",
       "      <td>-0.222487</td>\n",
       "      <td>0.158242</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>0.105584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182662</td>\n",
       "      <td>-0.612268</td>\n",
       "      <td>0.197305</td>\n",
       "      <td>0.174883</td>\n",
       "      <td>0.032497</td>\n",
       "      <td>0.099480</td>\n",
       "      <td>-0.026816</td>\n",
       "      <td>0.004199</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.239048</td>\n",
       "      <td>0.293284</td>\n",
       "      <td>0.086472</td>\n",
       "      <td>1.178277</td>\n",
       "      <td>0.373344</td>\n",
       "      <td>0.325596</td>\n",
       "      <td>0.119319</td>\n",
       "      <td>-0.025476</td>\n",
       "      <td>0.173211</td>\n",
       "      <td>-0.154186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148136</td>\n",
       "      <td>-0.111893</td>\n",
       "      <td>-0.173207</td>\n",
       "      <td>-0.714729</td>\n",
       "      <td>0.817360</td>\n",
       "      <td>-0.263843</td>\n",
       "      <td>0.047777</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.038370</td>\n",
       "      <td>0.127486</td>\n",
       "      <td>0.184456</td>\n",
       "      <td>1.109950</td>\n",
       "      <td>0.441699</td>\n",
       "      <td>0.945283</td>\n",
       "      <td>-0.036715</td>\n",
       "      <td>0.350995</td>\n",
       "      <td>0.118950</td>\n",
       "      <td>-0.243289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102520</td>\n",
       "      <td>0.605089</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.626463</td>\n",
       "      <td>0.479120</td>\n",
       "      <td>-0.166937</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.038370</td>\n",
       "      <td>0.127486</td>\n",
       "      <td>0.184456</td>\n",
       "      <td>1.109950</td>\n",
       "      <td>0.441699</td>\n",
       "      <td>0.945283</td>\n",
       "      <td>-0.036715</td>\n",
       "      <td>0.350995</td>\n",
       "      <td>0.118950</td>\n",
       "      <td>-0.243289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102520</td>\n",
       "      <td>0.605089</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.626463</td>\n",
       "      <td>0.479120</td>\n",
       "      <td>-0.166937</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.038370</td>\n",
       "      <td>0.127486</td>\n",
       "      <td>0.184456</td>\n",
       "      <td>1.109950</td>\n",
       "      <td>0.441699</td>\n",
       "      <td>0.945283</td>\n",
       "      <td>-0.036715</td>\n",
       "      <td>0.350995</td>\n",
       "      <td>0.118950</td>\n",
       "      <td>-0.243289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102520</td>\n",
       "      <td>0.605089</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.626463</td>\n",
       "      <td>0.479120</td>\n",
       "      <td>-0.166937</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.038370</td>\n",
       "      <td>0.127486</td>\n",
       "      <td>0.184456</td>\n",
       "      <td>1.109950</td>\n",
       "      <td>0.441699</td>\n",
       "      <td>0.945283</td>\n",
       "      <td>-0.036715</td>\n",
       "      <td>0.350995</td>\n",
       "      <td>0.118950</td>\n",
       "      <td>-0.243289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102520</td>\n",
       "      <td>0.605089</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.626463</td>\n",
       "      <td>0.479120</td>\n",
       "      <td>-0.166937</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.276134</td>\n",
       "      <td>0.211470</td>\n",
       "      <td>0.143001</td>\n",
       "      <td>0.370050</td>\n",
       "      <td>0.154748</td>\n",
       "      <td>-0.032255</td>\n",
       "      <td>-0.003030</td>\n",
       "      <td>-0.038887</td>\n",
       "      <td>0.030715</td>\n",
       "      <td>-0.121891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279654</td>\n",
       "      <td>-0.747704</td>\n",
       "      <td>0.056808</td>\n",
       "      <td>-0.759464</td>\n",
       "      <td>0.271641</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>-0.010770</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-0.238411</td>\n",
       "      <td>0.509263</td>\n",
       "      <td>1.952001</td>\n",
       "      <td>0.779066</td>\n",
       "      <td>-0.378940</td>\n",
       "      <td>0.251137</td>\n",
       "      <td>-0.072412</td>\n",
       "      <td>0.228755</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>-0.373814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217449</td>\n",
       "      <td>0.903777</td>\n",
       "      <td>-0.121349</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>-0.613854</td>\n",
       "      <td>0.575920</td>\n",
       "      <td>0.193592</td>\n",
       "      <td>0.165132</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.343711</td>\n",
       "      <td>1.118615</td>\n",
       "      <td>1.293386</td>\n",
       "      <td>0.064762</td>\n",
       "      <td>0.023481</td>\n",
       "      <td>-0.992741</td>\n",
       "      <td>0.695814</td>\n",
       "      <td>-0.064809</td>\n",
       "      <td>-0.373640</td>\n",
       "      <td>-0.506204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265392</td>\n",
       "      <td>-0.700550</td>\n",
       "      <td>-0.014280</td>\n",
       "      <td>0.330615</td>\n",
       "      <td>-0.165442</td>\n",
       "      <td>0.072342</td>\n",
       "      <td>0.244882</td>\n",
       "      <td>0.098048</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.195572</td>\n",
       "      <td>0.258858</td>\n",
       "      <td>0.635796</td>\n",
       "      <td>0.641257</td>\n",
       "      <td>-0.395081</td>\n",
       "      <td>-0.694667</td>\n",
       "      <td>0.034086</td>\n",
       "      <td>-0.124346</td>\n",
       "      <td>-0.078400</td>\n",
       "      <td>-0.128968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201249</td>\n",
       "      <td>-0.516925</td>\n",
       "      <td>0.199096</td>\n",
       "      <td>0.412552</td>\n",
       "      <td>0.122984</td>\n",
       "      <td>0.101940</td>\n",
       "      <td>-0.007846</td>\n",
       "      <td>0.020214</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.175094</td>\n",
       "      <td>0.408263</td>\n",
       "      <td>0.552145</td>\n",
       "      <td>1.255068</td>\n",
       "      <td>-0.196662</td>\n",
       "      <td>-0.565605</td>\n",
       "      <td>0.133973</td>\n",
       "      <td>-0.146202</td>\n",
       "      <td>-0.214155</td>\n",
       "      <td>-0.011310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130585</td>\n",
       "      <td>0.523640</td>\n",
       "      <td>-0.050125</td>\n",
       "      <td>0.448133</td>\n",
       "      <td>0.597867</td>\n",
       "      <td>-0.275067</td>\n",
       "      <td>0.043308</td>\n",
       "      <td>0.023924</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.175125</td>\n",
       "      <td>0.661341</td>\n",
       "      <td>-0.477476</td>\n",
       "      <td>1.102542</td>\n",
       "      <td>0.543328</td>\n",
       "      <td>-0.246785</td>\n",
       "      <td>0.165234</td>\n",
       "      <td>0.027018</td>\n",
       "      <td>-0.423144</td>\n",
       "      <td>-0.574065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>0.093738</td>\n",
       "      <td>-0.154355</td>\n",
       "      <td>-0.425097</td>\n",
       "      <td>0.652435</td>\n",
       "      <td>-0.296802</td>\n",
       "      <td>0.051425</td>\n",
       "      <td>0.041876</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.254914</td>\n",
       "      <td>0.350287</td>\n",
       "      <td>0.302488</td>\n",
       "      <td>0.693114</td>\n",
       "      <td>-0.371470</td>\n",
       "      <td>-1.070256</td>\n",
       "      <td>0.086781</td>\n",
       "      <td>-0.202836</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>-0.282617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287592</td>\n",
       "      <td>-0.832682</td>\n",
       "      <td>0.128083</td>\n",
       "      <td>0.339427</td>\n",
       "      <td>0.215944</td>\n",
       "      <td>0.094704</td>\n",
       "      <td>-0.023354</td>\n",
       "      <td>0.030892</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.175478</td>\n",
       "      <td>0.167881</td>\n",
       "      <td>0.509909</td>\n",
       "      <td>0.452332</td>\n",
       "      <td>-0.215763</td>\n",
       "      <td>-0.192104</td>\n",
       "      <td>-0.105404</td>\n",
       "      <td>0.068517</td>\n",
       "      <td>-0.249733</td>\n",
       "      <td>0.053143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176147</td>\n",
       "      <td>-0.500566</td>\n",
       "      <td>0.151263</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>0.122003</td>\n",
       "      <td>0.106307</td>\n",
       "      <td>-0.014937</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-0.399880</td>\n",
       "      <td>1.051966</td>\n",
       "      <td>1.390802</td>\n",
       "      <td>0.093132</td>\n",
       "      <td>-0.007699</td>\n",
       "      <td>-0.717559</td>\n",
       "      <td>0.588095</td>\n",
       "      <td>0.085932</td>\n",
       "      <td>-0.310722</td>\n",
       "      <td>-0.533285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224926</td>\n",
       "      <td>-0.573766</td>\n",
       "      <td>0.057041</td>\n",
       "      <td>0.329354</td>\n",
       "      <td>-0.302724</td>\n",
       "      <td>0.083883</td>\n",
       "      <td>0.258834</td>\n",
       "      <td>0.096337</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1.239173</td>\n",
       "      <td>0.103368</td>\n",
       "      <td>0.371711</td>\n",
       "      <td>0.440033</td>\n",
       "      <td>-0.370396</td>\n",
       "      <td>-0.509393</td>\n",
       "      <td>-0.129165</td>\n",
       "      <td>0.030558</td>\n",
       "      <td>-0.010578</td>\n",
       "      <td>0.138778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240934</td>\n",
       "      <td>-0.790563</td>\n",
       "      <td>0.097823</td>\n",
       "      <td>-0.034851</td>\n",
       "      <td>0.190347</td>\n",
       "      <td>0.101428</td>\n",
       "      <td>-0.041183</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>-0.409900</td>\n",
       "      <td>1.183088</td>\n",
       "      <td>1.598967</td>\n",
       "      <td>0.353088</td>\n",
       "      <td>0.309710</td>\n",
       "      <td>-0.312400</td>\n",
       "      <td>0.707197</td>\n",
       "      <td>-0.043206</td>\n",
       "      <td>-0.892869</td>\n",
       "      <td>-0.684800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163371</td>\n",
       "      <td>-0.396155</td>\n",
       "      <td>-0.069498</td>\n",
       "      <td>0.069735</td>\n",
       "      <td>-0.298407</td>\n",
       "      <td>0.199188</td>\n",
       "      <td>0.099692</td>\n",
       "      <td>0.118617</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1.240600</td>\n",
       "      <td>0.747735</td>\n",
       "      <td>-0.214136</td>\n",
       "      <td>1.301475</td>\n",
       "      <td>0.124740</td>\n",
       "      <td>-1.172183</td>\n",
       "      <td>0.371059</td>\n",
       "      <td>-0.306401</td>\n",
       "      <td>-0.246672</td>\n",
       "      <td>-0.597784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039413</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>-0.132261</td>\n",
       "      <td>0.306805</td>\n",
       "      <td>0.728251</td>\n",
       "      <td>-0.331239</td>\n",
       "      <td>0.039024</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1.255729</td>\n",
       "      <td>0.297650</td>\n",
       "      <td>0.287526</td>\n",
       "      <td>0.699902</td>\n",
       "      <td>-0.438405</td>\n",
       "      <td>-1.088542</td>\n",
       "      <td>0.034421</td>\n",
       "      <td>-0.159596</td>\n",
       "      <td>0.149990</td>\n",
       "      <td>-0.260028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296736</td>\n",
       "      <td>-0.894374</td>\n",
       "      <td>0.137806</td>\n",
       "      <td>0.318641</td>\n",
       "      <td>0.190562</td>\n",
       "      <td>0.097319</td>\n",
       "      <td>-0.028174</td>\n",
       "      <td>0.029141</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>-1.254476</td>\n",
       "      <td>0.322250</td>\n",
       "      <td>1.784507</td>\n",
       "      <td>-0.307396</td>\n",
       "      <td>1.009086</td>\n",
       "      <td>-0.998491</td>\n",
       "      <td>0.328482</td>\n",
       "      <td>0.131284</td>\n",
       "      <td>-0.485255</td>\n",
       "      <td>-1.028343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160980</td>\n",
       "      <td>-0.778999</td>\n",
       "      <td>-0.025359</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.154781</td>\n",
       "      <td>0.042034</td>\n",
       "      <td>-0.019550</td>\n",
       "      <td>0.081176</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>-1.299837</td>\n",
       "      <td>0.881817</td>\n",
       "      <td>1.452842</td>\n",
       "      <td>-1.293698</td>\n",
       "      <td>-0.025105</td>\n",
       "      <td>-1.170103</td>\n",
       "      <td>0.861610</td>\n",
       "      <td>-0.193934</td>\n",
       "      <td>0.592001</td>\n",
       "      <td>0.241979</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272563</td>\n",
       "      <td>-0.360853</td>\n",
       "      <td>0.223911</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>-0.397705</td>\n",
       "      <td>0.637141</td>\n",
       "      <td>0.234872</td>\n",
       "      <td>0.021379</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>1.213136</td>\n",
       "      <td>0.462143</td>\n",
       "      <td>0.664599</td>\n",
       "      <td>1.301135</td>\n",
       "      <td>-0.407416</td>\n",
       "      <td>-0.994125</td>\n",
       "      <td>0.180626</td>\n",
       "      <td>-0.279035</td>\n",
       "      <td>-0.216489</td>\n",
       "      <td>0.016012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069834</td>\n",
       "      <td>0.318994</td>\n",
       "      <td>-0.048026</td>\n",
       "      <td>0.758936</td>\n",
       "      <td>0.616221</td>\n",
       "      <td>-0.354057</td>\n",
       "      <td>0.032492</td>\n",
       "      <td>0.030264</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>-0.553092</td>\n",
       "      <td>1.667591</td>\n",
       "      <td>-0.047357</td>\n",
       "      <td>0.514249</td>\n",
       "      <td>0.589388</td>\n",
       "      <td>-0.635411</td>\n",
       "      <td>1.126611</td>\n",
       "      <td>-0.311882</td>\n",
       "      <td>0.035247</td>\n",
       "      <td>1.704936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033537</td>\n",
       "      <td>0.687658</td>\n",
       "      <td>-0.076693</td>\n",
       "      <td>0.014810</td>\n",
       "      <td>-0.590713</td>\n",
       "      <td>-0.482998</td>\n",
       "      <td>0.137717</td>\n",
       "      <td>-0.334915</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>-0.160626</td>\n",
       "      <td>-0.064459</td>\n",
       "      <td>2.531072</td>\n",
       "      <td>-1.328268</td>\n",
       "      <td>-0.970430</td>\n",
       "      <td>0.185031</td>\n",
       "      <td>-0.380184</td>\n",
       "      <td>0.078119</td>\n",
       "      <td>1.775956</td>\n",
       "      <td>-1.242315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262124</td>\n",
       "      <td>1.340696</td>\n",
       "      <td>-0.253860</td>\n",
       "      <td>0.178601</td>\n",
       "      <td>-0.491121</td>\n",
       "      <td>0.244972</td>\n",
       "      <td>0.074829</td>\n",
       "      <td>-0.069924</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1.237413</td>\n",
       "      <td>0.512365</td>\n",
       "      <td>0.687746</td>\n",
       "      <td>1.693872</td>\n",
       "      <td>-0.236323</td>\n",
       "      <td>-0.650232</td>\n",
       "      <td>0.118066</td>\n",
       "      <td>-0.230545</td>\n",
       "      <td>-0.808523</td>\n",
       "      <td>0.511284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077543</td>\n",
       "      <td>-0.178220</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.471218</td>\n",
       "      <td>0.289249</td>\n",
       "      <td>0.871803</td>\n",
       "      <td>-0.066884</td>\n",
       "      <td>0.012986</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>-0.814054</td>\n",
       "      <td>1.538222</td>\n",
       "      <td>1.115690</td>\n",
       "      <td>-0.051667</td>\n",
       "      <td>0.092334</td>\n",
       "      <td>-1.013398</td>\n",
       "      <td>0.748851</td>\n",
       "      <td>-0.124814</td>\n",
       "      <td>-0.207407</td>\n",
       "      <td>0.072842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311452</td>\n",
       "      <td>-0.627544</td>\n",
       "      <td>-0.016469</td>\n",
       "      <td>0.363403</td>\n",
       "      <td>-0.014631</td>\n",
       "      <td>0.076914</td>\n",
       "      <td>0.467478</td>\n",
       "      <td>0.228123</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>-0.544922</td>\n",
       "      <td>0.595407</td>\n",
       "      <td>1.813261</td>\n",
       "      <td>-1.344670</td>\n",
       "      <td>0.016864</td>\n",
       "      <td>-0.601398</td>\n",
       "      <td>0.660876</td>\n",
       "      <td>-0.058978</td>\n",
       "      <td>0.317033</td>\n",
       "      <td>-0.523190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123048</td>\n",
       "      <td>-0.148228</td>\n",
       "      <td>-0.076075</td>\n",
       "      <td>0.074036</td>\n",
       "      <td>-0.486633</td>\n",
       "      <td>0.724549</td>\n",
       "      <td>0.104294</td>\n",
       "      <td>-0.055110</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>-0.355221</td>\n",
       "      <td>1.155882</td>\n",
       "      <td>1.311865</td>\n",
       "      <td>0.061945</td>\n",
       "      <td>0.114207</td>\n",
       "      <td>-0.990220</td>\n",
       "      <td>0.728195</td>\n",
       "      <td>-0.105630</td>\n",
       "      <td>-0.486183</td>\n",
       "      <td>-0.525351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255803</td>\n",
       "      <td>-0.639619</td>\n",
       "      <td>-0.022676</td>\n",
       "      <td>0.351456</td>\n",
       "      <td>-0.139464</td>\n",
       "      <td>0.069852</td>\n",
       "      <td>0.249580</td>\n",
       "      <td>0.099988</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>-1.043068</td>\n",
       "      <td>1.044582</td>\n",
       "      <td>0.292186</td>\n",
       "      <td>-2.378536</td>\n",
       "      <td>-0.189793</td>\n",
       "      <td>-0.920526</td>\n",
       "      <td>0.178722</td>\n",
       "      <td>-0.384095</td>\n",
       "      <td>0.237234</td>\n",
       "      <td>-1.336920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934064</td>\n",
       "      <td>0.316389</td>\n",
       "      <td>-0.004709</td>\n",
       "      <td>0.268738</td>\n",
       "      <td>-0.177186</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>0.290259</td>\n",
       "      <td>0.127530</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>1.121785</td>\n",
       "      <td>0.206367</td>\n",
       "      <td>0.529086</td>\n",
       "      <td>1.318229</td>\n",
       "      <td>-0.161515</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>-0.020503</td>\n",
       "      <td>0.103401</td>\n",
       "      <td>-0.023411</td>\n",
       "      <td>0.039351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037897</td>\n",
       "      <td>0.121195</td>\n",
       "      <td>-0.046623</td>\n",
       "      <td>0.239376</td>\n",
       "      <td>0.617560</td>\n",
       "      <td>-0.342429</td>\n",
       "      <td>0.039618</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>-1.860258</td>\n",
       "      <td>-0.629859</td>\n",
       "      <td>0.966570</td>\n",
       "      <td>0.844632</td>\n",
       "      <td>0.759983</td>\n",
       "      <td>-1.481173</td>\n",
       "      <td>-0.509681</td>\n",
       "      <td>0.540722</td>\n",
       "      <td>-0.733623</td>\n",
       "      <td>-0.371622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268028</td>\n",
       "      <td>0.125515</td>\n",
       "      <td>-0.225029</td>\n",
       "      <td>0.586664</td>\n",
       "      <td>-0.031598</td>\n",
       "      <td>0.570168</td>\n",
       "      <td>-0.043007</td>\n",
       "      <td>-0.223739</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>-1.647248</td>\n",
       "      <td>1.627046</td>\n",
       "      <td>0.828098</td>\n",
       "      <td>-1.192708</td>\n",
       "      <td>-1.218394</td>\n",
       "      <td>-1.547978</td>\n",
       "      <td>-0.063065</td>\n",
       "      <td>0.635641</td>\n",
       "      <td>0.564945</td>\n",
       "      <td>0.052423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136868</td>\n",
       "      <td>-0.285386</td>\n",
       "      <td>0.149182</td>\n",
       "      <td>0.924095</td>\n",
       "      <td>-0.176509</td>\n",
       "      <td>0.682287</td>\n",
       "      <td>0.072508</td>\n",
       "      <td>-0.095067</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1.491574</td>\n",
       "      <td>-1.088278</td>\n",
       "      <td>0.552852</td>\n",
       "      <td>-1.289876</td>\n",
       "      <td>-1.391175</td>\n",
       "      <td>-0.101768</td>\n",
       "      <td>-1.156643</td>\n",
       "      <td>-0.023064</td>\n",
       "      <td>-1.706012</td>\n",
       "      <td>1.419577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224231</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>-0.090743</td>\n",
       "      <td>0.045121</td>\n",
       "      <td>0.593903</td>\n",
       "      <td>-0.100271</td>\n",
       "      <td>0.052103</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1.258778</td>\n",
       "      <td>-0.059313</td>\n",
       "      <td>0.314052</td>\n",
       "      <td>-0.111067</td>\n",
       "      <td>-0.503006</td>\n",
       "      <td>-0.704089</td>\n",
       "      <td>-0.156473</td>\n",
       "      <td>-0.051544</td>\n",
       "      <td>0.369292</td>\n",
       "      <td>-0.210176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092730</td>\n",
       "      <td>-0.249496</td>\n",
       "      <td>0.086934</td>\n",
       "      <td>0.124848</td>\n",
       "      <td>0.141429</td>\n",
       "      <td>0.992693</td>\n",
       "      <td>-0.072256</td>\n",
       "      <td>-0.001743</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>-0.627617</td>\n",
       "      <td>0.841974</td>\n",
       "      <td>0.840728</td>\n",
       "      <td>-1.351059</td>\n",
       "      <td>0.182650</td>\n",
       "      <td>-0.484541</td>\n",
       "      <td>0.465110</td>\n",
       "      <td>0.181788</td>\n",
       "      <td>-0.074016</td>\n",
       "      <td>-0.774767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022654</td>\n",
       "      <td>-0.101469</td>\n",
       "      <td>-0.083594</td>\n",
       "      <td>-0.387958</td>\n",
       "      <td>-0.298916</td>\n",
       "      <td>0.771190</td>\n",
       "      <td>-0.467113</td>\n",
       "      <td>-0.271084</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>1.179162</td>\n",
       "      <td>0.205327</td>\n",
       "      <td>0.704971</td>\n",
       "      <td>0.697812</td>\n",
       "      <td>-0.566141</td>\n",
       "      <td>-0.830944</td>\n",
       "      <td>-0.013824</td>\n",
       "      <td>-0.093148</td>\n",
       "      <td>0.026189</td>\n",
       "      <td>-0.106550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196312</td>\n",
       "      <td>-0.543615</td>\n",
       "      <td>0.233359</td>\n",
       "      <td>0.597346</td>\n",
       "      <td>0.068706</td>\n",
       "      <td>0.091725</td>\n",
       "      <td>-0.012834</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>-0.942973</td>\n",
       "      <td>1.812575</td>\n",
       "      <td>2.112754</td>\n",
       "      <td>2.211791</td>\n",
       "      <td>-0.067565</td>\n",
       "      <td>-0.614476</td>\n",
       "      <td>0.775533</td>\n",
       "      <td>-0.078299</td>\n",
       "      <td>-1.276690</td>\n",
       "      <td>0.939466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181812</td>\n",
       "      <td>-0.441114</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>0.881532</td>\n",
       "      <td>-0.169525</td>\n",
       "      <td>-0.213305</td>\n",
       "      <td>0.313371</td>\n",
       "      <td>0.246707</td>\n",
       "      <td>2.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>1.314054</td>\n",
       "      <td>0.433171</td>\n",
       "      <td>-0.577559</td>\n",
       "      <td>0.715883</td>\n",
       "      <td>0.683583</td>\n",
       "      <td>-0.194720</td>\n",
       "      <td>0.442210</td>\n",
       "      <td>-0.147779</td>\n",
       "      <td>-0.495027</td>\n",
       "      <td>0.208418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018596</td>\n",
       "      <td>-0.016909</td>\n",
       "      <td>-0.306040</td>\n",
       "      <td>-0.810191</td>\n",
       "      <td>0.986072</td>\n",
       "      <td>-0.242416</td>\n",
       "      <td>-0.011943</td>\n",
       "      <td>-0.010327</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>-0.634597</td>\n",
       "      <td>0.866354</td>\n",
       "      <td>1.123836</td>\n",
       "      <td>-0.304041</td>\n",
       "      <td>1.173910</td>\n",
       "      <td>0.257118</td>\n",
       "      <td>0.754325</td>\n",
       "      <td>0.171735</td>\n",
       "      <td>-0.825981</td>\n",
       "      <td>-0.531058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160463</td>\n",
       "      <td>0.422873</td>\n",
       "      <td>-0.353721</td>\n",
       "      <td>-0.847192</td>\n",
       "      <td>0.206896</td>\n",
       "      <td>-0.436112</td>\n",
       "      <td>0.110134</td>\n",
       "      <td>0.102261</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>-0.839473</td>\n",
       "      <td>1.349968</td>\n",
       "      <td>0.478163</td>\n",
       "      <td>0.507629</td>\n",
       "      <td>0.539551</td>\n",
       "      <td>0.430461</td>\n",
       "      <td>0.237864</td>\n",
       "      <td>0.589883</td>\n",
       "      <td>-0.896515</td>\n",
       "      <td>-0.012143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186384</td>\n",
       "      <td>0.583830</td>\n",
       "      <td>-0.137197</td>\n",
       "      <td>-0.836186</td>\n",
       "      <td>-0.138522</td>\n",
       "      <td>-0.259282</td>\n",
       "      <td>0.181760</td>\n",
       "      <td>0.130868</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>-1.100920</td>\n",
       "      <td>1.029588</td>\n",
       "      <td>1.348333</td>\n",
       "      <td>-1.362082</td>\n",
       "      <td>-0.343465</td>\n",
       "      <td>-0.671659</td>\n",
       "      <td>0.291222</td>\n",
       "      <td>0.379994</td>\n",
       "      <td>0.338839</td>\n",
       "      <td>-0.438421</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110388</td>\n",
       "      <td>-0.180427</td>\n",
       "      <td>-0.007197</td>\n",
       "      <td>0.068877</td>\n",
       "      <td>-0.410540</td>\n",
       "      <td>0.731643</td>\n",
       "      <td>0.084051</td>\n",
       "      <td>-0.057236</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>-0.867554</td>\n",
       "      <td>-0.418633</td>\n",
       "      <td>2.294927</td>\n",
       "      <td>0.219870</td>\n",
       "      <td>-0.085820</td>\n",
       "      <td>-0.839204</td>\n",
       "      <td>-0.724947</td>\n",
       "      <td>0.221812</td>\n",
       "      <td>0.423738</td>\n",
       "      <td>-0.431876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231306</td>\n",
       "      <td>0.547470</td>\n",
       "      <td>-0.001608</td>\n",
       "      <td>0.461131</td>\n",
       "      <td>-0.504834</td>\n",
       "      <td>1.146692</td>\n",
       "      <td>0.061651</td>\n",
       "      <td>0.146244</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>-0.394364</td>\n",
       "      <td>0.962489</td>\n",
       "      <td>1.164111</td>\n",
       "      <td>-0.110028</td>\n",
       "      <td>0.221216</td>\n",
       "      <td>-0.526324</td>\n",
       "      <td>0.505040</td>\n",
       "      <td>0.147743</td>\n",
       "      <td>-0.499457</td>\n",
       "      <td>-0.311499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240069</td>\n",
       "      <td>-0.709400</td>\n",
       "      <td>-0.051337</td>\n",
       "      <td>-0.071240</td>\n",
       "      <td>-0.173978</td>\n",
       "      <td>0.077661</td>\n",
       "      <td>0.234553</td>\n",
       "      <td>0.083875</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>-0.928088</td>\n",
       "      <td>0.398194</td>\n",
       "      <td>1.741131</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.966387</td>\n",
       "      <td>-0.901004</td>\n",
       "      <td>0.879016</td>\n",
       "      <td>-0.156590</td>\n",
       "      <td>-0.142117</td>\n",
       "      <td>-0.574775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066353</td>\n",
       "      <td>0.281378</td>\n",
       "      <td>-0.257966</td>\n",
       "      <td>0.385384</td>\n",
       "      <td>0.391117</td>\n",
       "      <td>-0.453853</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.125765</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1.255439</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.699873</td>\n",
       "      <td>-0.428876</td>\n",
       "      <td>-1.088456</td>\n",
       "      <td>0.043840</td>\n",
       "      <td>-0.167739</td>\n",
       "      <td>0.128854</td>\n",
       "      <td>-0.264062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294795</td>\n",
       "      <td>-0.882126</td>\n",
       "      <td>0.136846</td>\n",
       "      <td>0.327949</td>\n",
       "      <td>0.194459</td>\n",
       "      <td>0.096516</td>\n",
       "      <td>-0.027271</td>\n",
       "      <td>0.029491</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3         4         5         6         7   \\\n",
       "0    1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "13  -0.436905  0.918966  0.924591 -0.727219  0.915679 -0.127867  0.707642   \n",
       "18   1.166616  0.502120 -0.067300  2.261569  0.428804  0.089474  0.241147   \n",
       "22  -1.946525 -0.044901 -0.405570 -1.013057  2.941968  2.955053 -0.063063   \n",
       "26  -0.535388  0.865268  1.351076  0.147575  0.433680  0.086983  0.693039   \n",
       "26  -0.535388  0.865268  1.351076  0.147575  0.433680  0.086983  0.693039   \n",
       "27  -1.452187  1.765124  0.611669  1.176825 -0.445980  0.246826 -0.257566   \n",
       "32  -2.008872  2.198527  0.144242  1.159432 -0.815174  0.182288 -0.617108   \n",
       "35   1.199356  0.129953  0.863585  1.002635 -0.783761 -0.884679 -0.040743   \n",
       "36   1.095525 -0.116085  1.397912  1.497547 -1.049124  0.072839 -0.723802   \n",
       "37   1.295668  0.341483  0.081505  0.566746 -0.110459 -0.766325  0.073155   \n",
       "41   1.154312  0.265462  0.384871  0.575007 -0.217475 -0.391520 -0.081489   \n",
       "44  -0.948896  0.248414  2.956914  2.813750  0.145539 -0.027353  0.133702   \n",
       "51   1.259873  0.254240  0.514789  0.620924 -0.475930 -0.992286  0.066417   \n",
       "55  -0.773450  0.853112  0.818254 -0.236070  0.803463 -1.438728  0.799479   \n",
       "68   1.156939  0.037215  0.556799  0.519507 -0.479754 -0.352714 -0.222487   \n",
       "73   1.239048  0.293284  0.086472  1.178277  0.373344  0.325596  0.119319   \n",
       "74   1.038370  0.127486  0.184456  1.109950  0.441699  0.945283 -0.036715   \n",
       "74   1.038370  0.127486  0.184456  1.109950  0.441699  0.945283 -0.036715   \n",
       "74   1.038370  0.127486  0.184456  1.109950  0.441699  0.945283 -0.036715   \n",
       "74   1.038370  0.127486  0.184456  1.109950  0.441699  0.945283 -0.036715   \n",
       "91   1.276134  0.211470  0.143001  0.370050  0.154748 -0.032255 -0.003030   \n",
       "92  -0.238411  0.509263  1.952001  0.779066 -0.378940  0.251137 -0.072412   \n",
       "95  -0.343711  1.118615  1.293386  0.064762  0.023481 -0.992741  0.695814   \n",
       "95   1.195572  0.258858  0.635796  0.641257 -0.395081 -0.694667  0.034086   \n",
       "105  1.175094  0.408263  0.552145  1.255068 -0.196662 -0.565605  0.133973   \n",
       "107  1.175125  0.661341 -0.477476  1.102542  0.543328 -0.246785  0.165234   \n",
       "118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256  0.086781   \n",
       "140  1.175478  0.167881  0.509909  0.452332 -0.215763 -0.192104 -0.105404   \n",
       "150 -0.399880  1.051966  1.390802  0.093132 -0.007699 -0.717559  0.588095   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "368  1.239173  0.103368  0.371711  0.440033 -0.370396 -0.509393 -0.129165   \n",
       "368 -0.409900  1.183088  1.598967  0.353088  0.309710 -0.312400  0.707197   \n",
       "369  1.240600  0.747735 -0.214136  1.301475  0.124740 -1.172183  0.371059   \n",
       "369  1.255729  0.297650  0.287526  0.699902 -0.438405 -1.088542  0.034421   \n",
       "373 -1.254476  0.322250  1.784507 -0.307396  1.009086 -0.998491  0.328482   \n",
       "380 -1.299837  0.881817  1.452842 -1.293698 -0.025105 -1.170103  0.861610   \n",
       "386  1.213136  0.462143  0.664599  1.301135 -0.407416 -0.994125  0.180626   \n",
       "394 -0.553092  1.667591 -0.047357  0.514249  0.589388 -0.635411  1.126611   \n",
       "402 -0.160626 -0.064459  2.531072 -1.328268 -0.970430  0.185031 -0.380184   \n",
       "403  1.237413  0.512365  0.687746  1.693872 -0.236323 -0.650232  0.118066   \n",
       "406 -0.814054  1.538222  1.115690 -0.051667  0.092334 -1.013398  0.748851   \n",
       "409 -0.544922  0.595407  1.813261 -1.344670  0.016864 -0.601398  0.660876   \n",
       "410 -0.355221  1.155882  1.311865  0.061945  0.114207 -0.990220  0.728195   \n",
       "414 -1.043068  1.044582  0.292186 -2.378536 -0.189793 -0.920526  0.178722   \n",
       "421  1.121785  0.206367  0.529086  1.318229 -0.161515  0.024067 -0.020503   \n",
       "430 -1.860258 -0.629859  0.966570  0.844632  0.759983 -1.481173 -0.509681   \n",
       "430 -1.647248  1.627046  0.828098 -1.192708 -1.218394 -1.547978 -0.063065   \n",
       "438  1.491574 -1.088278  0.552852 -1.289876 -1.391175 -0.101768 -1.156643   \n",
       "439  1.258778 -0.059313  0.314052 -0.111067 -0.503006 -0.704089 -0.156473   \n",
       "442 -0.627617  0.841974  0.840728 -1.351059  0.182650 -0.484541  0.465110   \n",
       "455  1.179162  0.205327  0.704971  0.697812 -0.566141 -0.830944 -0.013824   \n",
       "457 -0.942973  1.812575  2.112754  2.211791 -0.067565 -0.614476  0.775533   \n",
       "460  1.314054  0.433171 -0.577559  0.715883  0.683583 -0.194720  0.442210   \n",
       "463 -0.634597  0.866354  1.123836 -0.304041  1.173910  0.257118  0.754325   \n",
       "464 -0.839473  1.349968  0.478163  0.507629  0.539551  0.430461  0.237864   \n",
       "472 -1.100920  1.029588  1.348333 -1.362082 -0.343465 -0.671659  0.291222   \n",
       "476 -0.867554 -0.418633  2.294927  0.219870 -0.085820 -0.839204 -0.724947   \n",
       "478 -0.394364  0.962489  1.164111 -0.110028  0.221216 -0.526324  0.505040   \n",
       "484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004  0.879016   \n",
       "499  1.255439  0.307729  0.292700  0.699873 -0.428876 -1.088456  0.043840   \n",
       "\n",
       "           8         9         10  ...        21        22        23  \\\n",
       "0    0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288   \n",
       "13   0.087962 -0.665271 -0.737980  ... -0.194796 -0.672638 -0.156858   \n",
       "18   0.138082 -0.989162  0.922175  ...  0.018702 -0.061972 -0.103855   \n",
       "22   0.855546  0.049967  0.573743  ... -0.579526 -0.799229  0.870300   \n",
       "26   0.179742 -0.285642 -0.482474  ...  0.049526  0.206537 -0.187108   \n",
       "26   0.179742 -0.285642 -0.482474  ...  0.049526  0.206537 -0.187108   \n",
       "27   1.092472 -0.607524  0.047156  ...  0.082280  0.325782 -0.069107   \n",
       "32   1.530817 -0.586832  0.129876  ...  0.094917  0.294983  0.011081   \n",
       "35  -0.208069  0.392478 -0.248178  ... -0.042468  0.198474 -0.033010   \n",
       "36   0.287532  0.996327 -0.149145  ... -0.033234  0.093262  0.089498   \n",
       "37  -0.168304  0.071837 -0.281044  ... -0.323607 -0.929781  0.063809   \n",
       "41   0.062789 -0.260583 -0.161677  ... -0.193213 -0.557685  0.169492   \n",
       "44  -0.307535 -0.125244  1.034940  ... -0.083647  0.416090  0.207537   \n",
       "51  -0.209275  0.035572 -0.067616  ... -0.256531 -0.739212  0.135732   \n",
       "55  -0.007989 -0.761090 -1.044464  ...  0.035362 -0.116890 -0.178926   \n",
       "68   0.158242  0.011252  0.105584  ... -0.182662 -0.612268  0.197305   \n",
       "73  -0.025476  0.173211 -0.154186  ... -0.148136 -0.111893 -0.173207   \n",
       "74   0.350995  0.118950 -0.243289  ...  0.102520  0.605089  0.023092   \n",
       "74   0.350995  0.118950 -0.243289  ...  0.102520  0.605089  0.023092   \n",
       "74   0.350995  0.118950 -0.243289  ...  0.102520  0.605089  0.023092   \n",
       "74   0.350995  0.118950 -0.243289  ...  0.102520  0.605089  0.023092   \n",
       "91  -0.038887  0.030715 -0.121891  ... -0.279654 -0.747704  0.056808   \n",
       "92   0.228755  0.210900 -0.373814  ...  0.217449  0.903777 -0.121349   \n",
       "95  -0.064809 -0.373640 -0.506204  ... -0.265392 -0.700550 -0.014280   \n",
       "95  -0.124346 -0.078400 -0.128968  ... -0.201249 -0.516925  0.199096   \n",
       "105 -0.146202 -0.214155 -0.011310  ...  0.130585  0.523640 -0.050125   \n",
       "107  0.027018 -0.423144 -0.574065  ... -0.003071  0.093738 -0.154355   \n",
       "118 -0.202836  0.035154 -0.282617  ... -0.287592 -0.832682  0.128083   \n",
       "140  0.068517 -0.249733  0.053143  ... -0.176147 -0.500566  0.151263   \n",
       "150  0.085932 -0.310722 -0.533285  ... -0.224926 -0.573766  0.057041   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "368  0.030558 -0.010578  0.138778  ... -0.240934 -0.790563  0.097823   \n",
       "368 -0.043206 -0.892869 -0.684800  ... -0.163371 -0.396155 -0.069498   \n",
       "369 -0.306401 -0.246672 -0.597784  ... -0.039413  0.010700 -0.132261   \n",
       "369 -0.159596  0.149990 -0.260028  ... -0.296736 -0.894374  0.137806   \n",
       "373  0.131284 -0.485255 -1.028343  ... -0.160980 -0.778999 -0.025359   \n",
       "380 -0.193934  0.592001  0.241979  ... -0.272563 -0.360853  0.223911   \n",
       "386 -0.279035 -0.216489  0.016012  ...  0.069834  0.318994 -0.048026   \n",
       "394 -0.311882  0.035247  1.704936  ... -0.033537  0.687658 -0.076693   \n",
       "402  0.078119  1.775956 -1.242315  ...  0.262124  1.340696 -0.253860   \n",
       "403 -0.230545 -0.808523  0.511284  ... -0.077543 -0.178220  0.038722   \n",
       "406 -0.124814 -0.207407  0.072842  ... -0.311452 -0.627544 -0.016469   \n",
       "409 -0.058978  0.317033 -0.523190  ... -0.123048 -0.148228 -0.076075   \n",
       "410 -0.105630 -0.486183 -0.525351  ... -0.255803 -0.639619 -0.022676   \n",
       "414 -0.384095  0.237234 -1.336920  ...  0.934064  0.316389 -0.004709   \n",
       "421  0.103401 -0.023411  0.039351  ... -0.037897  0.121195 -0.046623   \n",
       "430  0.540722 -0.733623 -0.371622  ...  0.268028  0.125515 -0.225029   \n",
       "430  0.635641  0.564945  0.052423  ... -0.136868 -0.285386  0.149182   \n",
       "438 -0.023064 -1.706012  1.419577  ... -0.224231  0.008611 -0.090743   \n",
       "439 -0.051544  0.369292 -0.210176  ... -0.092730 -0.249496  0.086934   \n",
       "442  0.181788 -0.074016 -0.774767  ... -0.022654 -0.101469 -0.083594   \n",
       "455 -0.093148  0.026189 -0.106550  ... -0.196312 -0.543615  0.233359   \n",
       "457 -0.078299 -1.276690  0.939466  ... -0.181812 -0.441114  0.004498   \n",
       "460 -0.147779 -0.495027  0.208418  ... -0.018596 -0.016909 -0.306040   \n",
       "463  0.171735 -0.825981 -0.531058  ...  0.160463  0.422873 -0.353721   \n",
       "464  0.589883 -0.896515 -0.012143  ...  0.186384  0.583830 -0.137197   \n",
       "472  0.379994  0.338839 -0.438421  ... -0.110388 -0.180427 -0.007197   \n",
       "476  0.221812  0.423738 -0.431876  ...  0.231306  0.547470 -0.001608   \n",
       "478  0.147743 -0.499457 -0.311499  ... -0.240069 -0.709400 -0.051337   \n",
       "484 -0.156590 -0.142117 -0.574775  ...  0.066353  0.281378 -0.257966   \n",
       "499 -0.167739  0.128854 -0.264062  ... -0.294795 -0.882126  0.136846   \n",
       "\n",
       "           24        25        26        27        28    29  30  \n",
       "0   -0.339846  0.167170  0.125895 -0.008983  0.014724  2.69   0  \n",
       "13  -0.888386 -0.342413 -0.049027  0.079692  0.131024  0.89   0  \n",
       "18  -0.370415  0.603200  0.108556 -0.040521 -0.011418  2.28   0  \n",
       "22   0.983421  0.321201  0.149650  0.707519  0.014600  0.89   0  \n",
       "26   0.000753  0.098117 -0.553471 -0.078306  0.025427  1.77   0  \n",
       "26   0.000753  0.098117 -0.553471 -0.078306  0.025427  1.77   0  \n",
       "27   0.020962 -0.044668 -0.243441  0.149180  0.120557  1.80   0  \n",
       "32   0.015249  0.034211 -0.236141  0.128291  0.117986  2.35   0  \n",
       "35   1.013290  0.559098  0.401818 -0.005865  0.017936  0.99   0  \n",
       "36   0.361261  0.250963 -0.378280  0.081024  0.034227  2.09   0  \n",
       "37  -0.193565  0.287574  0.127881 -0.023731  0.025200  0.99   0  \n",
       "41   0.186863  0.089252  0.093626 -0.009633  0.019668  2.67   0  \n",
       "44   0.716064 -0.602311 -0.064230 -0.315058 -0.272463  0.75   0  \n",
       "51   0.381990  0.219302  0.094291 -0.028687  0.019198  1.98   0  \n",
       "55   0.400155 -0.026231  0.165156  0.027762  0.132980  0.76   0  \n",
       "68   0.174883  0.032497  0.099480 -0.026816  0.004199  2.69   0  \n",
       "73  -0.714729  0.817360 -0.263843  0.047777  0.008735  1.00   0  \n",
       "74  -0.626463  0.479120 -0.166937  0.081247  0.001192  1.18   0  \n",
       "74  -0.626463  0.479120 -0.166937  0.081247  0.001192  1.18   0  \n",
       "74  -0.626463  0.479120 -0.166937  0.081247  0.001192  1.18   0  \n",
       "74  -0.626463  0.479120 -0.166937  0.081247  0.001192  1.18   0  \n",
       "91  -0.759464  0.271641  0.174757 -0.010770  0.008104  1.98   0  \n",
       "92   0.141225 -0.613854  0.575920  0.193592  0.165132  2.99   0  \n",
       "95   0.330615 -0.165442  0.072342  0.244882  0.098048  2.28   0  \n",
       "95   0.412552  0.122984  0.101940 -0.007846  0.020214  1.29   0  \n",
       "105  0.448133  0.597867 -0.275067  0.043308  0.023924  1.00   0  \n",
       "107 -0.425097  0.652435 -0.296802  0.051425  0.041876  1.00   0  \n",
       "118  0.339427  0.215944  0.094704 -0.023354  0.030892  2.69   0  \n",
       "140  0.018347  0.122003  0.106307 -0.014937  0.005771  1.29   0  \n",
       "150  0.329354 -0.302724  0.083883  0.258834  0.096337  0.89   0  \n",
       "..        ...       ...       ...       ...       ...   ...  ..  \n",
       "368 -0.034851  0.190347  0.101428 -0.041183  0.002979  1.98   0  \n",
       "368  0.069735 -0.298407  0.199188  0.099692  0.118617  1.98   0  \n",
       "369  0.306805  0.728251 -0.331239  0.039024  0.053600  1.00   0  \n",
       "369  0.318641  0.190562  0.097319 -0.028174  0.029141  1.98   0  \n",
       "373  0.000828  0.154781  0.042034 -0.019550  0.081176  1.79   0  \n",
       "380  0.598930 -0.397705  0.637141  0.234872  0.021379  0.00   0  \n",
       "386  0.758936  0.616221 -0.354057  0.032492  0.030264  1.00   0  \n",
       "394  0.014810 -0.590713 -0.482998  0.137717 -0.334915  1.55   0  \n",
       "402  0.178601 -0.491121  0.244972  0.074829 -0.069924  0.01   1  \n",
       "403  0.471218  0.289249  0.871803 -0.066884  0.012986  0.00   0  \n",
       "406  0.363403 -0.014631  0.076914  0.467478  0.228123  1.98   0  \n",
       "409  0.074036 -0.486633  0.724549  0.104294 -0.055110  0.77   0  \n",
       "410  0.351456 -0.139464  0.069852  0.249580  0.099988  0.89   0  \n",
       "414  0.268738 -0.177186 -0.126936  0.290259  0.127530  1.00   1  \n",
       "421  0.239376  0.617560 -0.342429  0.039618  0.007107  1.00   0  \n",
       "430  0.586664 -0.031598  0.570168 -0.043007 -0.223739  0.00   0  \n",
       "430  0.924095 -0.176509  0.682287  0.072508 -0.095067  2.31   0  \n",
       "438  0.045121  0.593903 -0.100271  0.052103  0.004537  2.00   0  \n",
       "439  0.124848  0.141429  0.992693 -0.072256 -0.001743  0.77   0  \n",
       "442 -0.387958 -0.298916  0.771190 -0.467113 -0.271084  2.23   0  \n",
       "455  0.597346  0.068706  0.091725 -0.012834  0.020738  1.98   0  \n",
       "457  0.881532 -0.169525 -0.213305  0.313371  0.246707  2.72   0  \n",
       "460 -0.810191  0.986072 -0.242416 -0.011943 -0.010327  2.85   0  \n",
       "463 -0.847192  0.206896 -0.436112  0.110134  0.102261  1.00   0  \n",
       "464 -0.836186 -0.138522 -0.259282  0.181760  0.130868  1.48   1  \n",
       "472  0.068877 -0.410540  0.731643  0.084051 -0.057236  0.92   0  \n",
       "476  0.461131 -0.504834  1.146692  0.061651  0.146244  1.00   0  \n",
       "478 -0.071240 -0.173978  0.077661  0.234553  0.083875  1.79   0  \n",
       "484  0.385384  0.391117 -0.453853 -0.104448 -0.125765  1.00   0  \n",
       "499  0.327949  0.194459  0.096516 -0.027271  0.029491  1.98   0  \n",
       "\n",
       "[106 rows x 30 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
